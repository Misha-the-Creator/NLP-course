{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b40a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import opencorpora\n",
    "from collections import Counter\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fcd924",
   "metadata": {},
   "source": [
    "# Блок обучения НС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517723d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение .conllu с получением токенов и pos-тэгов к ним\n",
    "\n",
    "def extract_sentences_and_pos_from_file(path):\n",
    "    sentences = []\n",
    "    pos_tags = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        current_sentence = []\n",
    "        current_pos = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            if not line:\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    pos_tags.append(current_pos)\n",
    "                    current_sentence = []\n",
    "                    current_pos = []\n",
    "                continue\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) >= 4:\n",
    "                word = parts[1]  # слово\n",
    "                pos = parts[3]   # часть речи\n",
    "                current_sentence.append(word)\n",
    "                current_pos.append(pos)\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "            pos_tags.append(current_pos)\n",
    "    return sentences, pos_tags\n",
    "###########################################################################\n",
    "\n",
    "sentences, pos_tags = extract_sentences_and_pos_from_file('ru_syntagrus-ud-train-b.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd9f9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for sent in sentences for word in sent]\n",
    "all_unique_tags = sorted(set(tag for tag_seq in pos_tags for tag in tag_seq))\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "MIN_FREQ = 2\n",
    "vocab_words = [word for word, cnt in word_counts.items() if cnt >= MIN_FREQ]\n",
    "\n",
    "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, word in enumerate(vocab_words, start=2):\n",
    "    word2idx[word] = i\n",
    "\n",
    "tag2idx = {tag: i for i, tag in enumerate(all_unique_tags)}\n",
    "tag2idx[\"<PAD>\"] = len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c486f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences, tags, word2idx, tag2idx):\n",
    "    X, Y = [], []\n",
    "    unk_id = word2idx[\"<UNK>\"]\n",
    "    \n",
    "    for sent, tag_seq in zip(sentences, tags):\n",
    "        x = [word2idx.get(word, unk_id) for word in sent]\n",
    "        y = [tag2idx[tag] for tag in tag_seq]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "X, Y = encode_sentences(sentences, pos_tags, word2idx, tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7318fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 0\n",
    "for sentence in sentences:\n",
    "    if max_len < len(sentence):\n",
    "        max_len = len(sentence)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "MAX_LEN = max_len\n",
    "\n",
    "X_padded = pad_sequences(X, maxlen=MAX_LEN, padding='post', value=word2idx[\"<PAD>\"])\n",
    "y_padded = pad_sequences(Y, maxlen=MAX_LEN, padding='post', value=tag2idx[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf0b9c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.array(X_padded)\n",
    "y_final = np.array(y_padded).reshape(-1, MAX_LEN, 1)  # для sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "859f55be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759694108.785012   50039 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4778 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 22:55:14.559219: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 29ms/step - accuracy: 0.0705 - loss: 0.6534 - val_accuracy: 0.0854 - val_loss: 0.2634\n",
      "Epoch 2/10\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.0843 - loss: 0.1535 - val_accuracy: 0.0873 - val_loss: 0.2122\n",
      "Epoch 3/10\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.0855 - loss: 0.1120 - val_accuracy: 0.0868 - val_loss: 0.2323\n",
      "Epoch 4/10\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.0863 - loss: 0.0867 - val_accuracy: 0.0870 - val_loss: 0.2323\n",
      "Epoch 5/10\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.0869 - loss: 0.0667 - val_accuracy: 0.0870 - val_loss: 0.2540\n",
      "Epoch 6/10\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 26ms/step - accuracy: 0.0874 - loss: 0.0509 - val_accuracy: 0.0865 - val_loss: 0.2902\n",
      "Epoch 7/10\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 27ms/step - accuracy: 0.0878 - loss: 0.0378 - val_accuracy: 0.0862 - val_loss: 0.3187\n",
      "Epoch 8/10\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 27ms/step - accuracy: 0.0881 - loss: 0.0279 - val_accuracy: 0.0861 - val_loss: 0.3582\n",
      "Epoch 9/10\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 25ms/step - accuracy: 0.0883 - loss: 0.0206 - val_accuracy: 0.0860 - val_loss: 0.3888\n",
      "Epoch 10/10\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.0885 - loss: 0.0155 - val_accuracy: 0.0858 - val_loss: 0.4238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7921f1ffcee0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, TimeDistributed\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "num_tags = len(tag2idx)\n",
    "\n",
    "inputs = Input(shape=(MAX_LEN,))\n",
    "embed = Embedding(vocab_size, 100, mask_zero=True)(inputs)\n",
    "bilstm = Bidirectional(LSTM(128, return_sequences=True))(embed)\n",
    "outputs = TimeDistributed(Dense(num_tags, activation='softmax'))(bilstm)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X_final, y_final, batch_size=32, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38bb2ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "[('Он', 'PRON'), ('любит', 'VERB'), ('печь', 'NOUN'), ('блины', 'NOUN'), ('и', 'CCONJ'), ('печь', 'NOUN'), ('блины', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# Возьмём ваше предложение\n",
    "test_sent = ['Он', 'любит', 'печь', 'блины', 'и', 'печь', 'блины']\n",
    "\n",
    "# Преобразуем в ID\n",
    "x_test = [word2idx.get(w, word2idx[\"<UNK>\"]) for w in test_sent]\n",
    "x_test = pad_sequences([x_test], maxlen=MAX_LEN, padding='post', value=word2idx[\"<PAD>\"])\n",
    "\n",
    "# Предсказание\n",
    "pred = model.predict(x_test)\n",
    "pred_ids = pred[0].argmax(axis=-1)\n",
    "\n",
    "# Тогда создайте обратный словарь:\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "\n",
    "# Обратно в теги\n",
    "pred_tags = [idx2tag[idx] for idx in pred_ids[:len(test_sent)]]\n",
    "print(list(zip(test_sent, pred_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a279b7",
   "metadata": {},
   "source": [
    "# Блок лемматизации + POS-тэггинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446b5032",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error reading file 'annot.opcorpora.xml': failed to load \"annot.opcorpora.xml\": No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[43mopencorpora\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mannot.opcorpora.xml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NLP-course/venv/lib/python3.10/site-packages/opencorpora/reader_lxml.py:24\u001b[0m, in \u001b[0;36mload\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mLoad OpenCorpora corpus.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m parser \u001b[38;5;241m=\u001b[39m get_xml_parser()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43metree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgetroot()\n",
      "File \u001b[0;32msrc/lxml/etree.pyx:3711\u001b[0m, in \u001b[0;36mlxml.etree.parse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parser.pxi:2031\u001b[0m, in \u001b[0;36mlxml.etree._parseDocument\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parser.pxi:2057\u001b[0m, in \u001b[0;36mlxml.etree._parseDocumentFromURL\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parser.pxi:1958\u001b[0m, in \u001b[0;36mlxml.etree._parseDocFromFile\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parser.pxi:1230\u001b[0m, in \u001b[0;36mlxml.etree._BaseParser._parseDocFromFile\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parser.pxi:647\u001b[0m, in \u001b[0;36mlxml.etree._ParserContext._handleParseResultDoc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parser.pxi:765\u001b[0m, in \u001b[0;36mlxml.etree._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/lxml/parser.pxi:687\u001b[0m, in \u001b[0;36mlxml.etree._raiseParseError\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Error reading file 'annot.opcorpora.xml': failed to load \"annot.opcorpora.xml\": No such file or directory"
     ]
    }
   ],
   "source": [
    "corpus = opencorpora.load('annot.opcorpora.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_to_remove = [',', '.', '?', '!', '\\n']\n",
    "\n",
    "with open('text.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in symbols_to_remove:\n",
    "    if char == '\\n':\n",
    "        text = text.replace(f'{char}', ' ')\n",
    "    else:\n",
    "        text = text.replace(f'{char}', '')\n",
    "\n",
    "splitted_text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tokens = [token.source for token in corpus.tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff7597e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 s, sys: 1.21 s, total: 26.9 s\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "counter = 0\n",
    "word_lemma_gramema = []\n",
    "\n",
    "for word in splitted_text:\n",
    "    for p in range(len(list_of_tokens)):\n",
    "        if word == list_of_tokens[p]:\n",
    "            finded_token = corpus.tokens[p]\n",
    "            word_lemma_gramema.append(f'{word}({finded_token.lemma}={finded_token.grammemes[0]})')\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter == len(list_of_tokens):\n",
    "                word_lemma_gramema.append(f'{word}(не_нашел_лемму=не_нашел_грамемму)')   \n",
    "    counter = 0\n",
    "\n",
    "joined = ' '.join(word_lemma_gramema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1be451fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Стала(стал=VERB) стабильнее(не_нашел_лемму=не_нашел_грамемму) экономическая(экономический=ADJF) и(и=CONJ) политическая(политический=ADJF) обстановка(обстановка=NOUN) предприятия(предприятие=NOUN) вывели(вывел=VERB) из(из=PREP) тени(тень=NOUN) зарплаты(зарплата=NOUN) сотрудников(сотрудник=NOUN) Все(весь=ADJF) Гришины(гришин=ADJF) одноклассники(одноклассник=NOUN) уже(уже=ADVB) побывали(побывал=VERB) за(за=PREP) границей(граница=NOUN) он(он=NPRO) был(есть=VERB) чуть(чуть=ADVB) ли(ли=PRCL) не(не=PRCL) единственным(единственный=ADJF) кого(кто=NPRO) не(не=PRCL) вывозили(вывожу=VERB) никуда(никуда=ADVB) дальше(дальше=COMP) Красной(красный=ADJF) Пахры(не_нашел_лемму=не_нашел_грамемму)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
